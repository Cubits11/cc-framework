# Correlation Cliff Experiment

**Status**: üü¢ Production-ready | **Rigor Level**: PhD-grade | **Runtime**: ~5-15 min (CPU-bound)

---

## TL;DR: What This Experiment Does

You have two AI guardrails (A and B) that flag unsafe content. You want to combine them with a rule like AND or OR. **The hidden danger**: if their failures correlate, the composition can make your system WORSE than using just the best single guardrail.

This experiment **discovers and quantifies the "correlation cliff"** - the exact dependence threshold (œÅ\* or Œª\*) where composition flips from constructive (safer) to destructive (less safe).

**Key outputs**:
- üìä **Phase diagram**: CC vs correlation with the cliff clearly visible
- üìà **Threshold estimate**: Œª\* ‚âà 0.47 (example) with 95% CI [0.44, 0.51]
- ‚úÖ **FH bounds validation**: All empirical points within theoretical envelope (0 violations)
- üéØ **Actionable insight**: "Don't compose these rails if œÅ > 0.47"

---

## Quick Start (One Command)

```bash
# From experiments/correlation_cliff/
python run_all.py --config config_s1.yaml

# Outputs written to: experiments/correlation_cliff/artifacts/<timestamp>/
# Key files:
#   - population_curve.csv       (theory/deterministic curve)
#   - sim_long.csv               (replicate-level data)
#   - sim_summary.csv            (per-lambda aggregates with CIs)
#   - thresholds.json            (Œª* estimates)
#   - figures/cc_vs_dependence.pdf    (THE MONEY PLOT)
#   - manifest.json              (reproducibility metadata)
```

**First time setup**:
```bash
pip install numpy pandas matplotlib pyyaml  # core deps
pip install scipy  # optional, for gaussian_copula path
```

**What you'll see** (if using default config_s1.yaml with S1 marginals):
- ~201 lambda points from 0.0 to 1.0
- ~20,000 samples per world per lambda (multinomial draws)
- BCa bootstrap with 2000 replications
- Runtime: ~8 minutes on modern CPU (single-threaded by design for reproducibility)

---

## Scientific Context

### The Problem

**Industry assumption (WRONG)**: "More guardrails = safer system"

**Reality**: Guardrail composition performance depends on **dependence structure** between failures:
- If failures are **anti-correlated** (one fails when the other succeeds): composition is **constructive** (CC < 1)
- If failures are **independent**: composition performance is predictable
- If failures are **correlated** (both fail together): composition is **destructive** (CC > 1)

### Why This Matters

1. **Nobody measures this**: Industry evaluates guardrails in isolation, then composes them blindly
2. **The risk is hidden**: Two "90% accurate" guardrails composed with AND can perform WORSE than either alone if their failures overlap
3. **No prior art**: This is the first quantitative framework for measuring compositional safety under unknown dependence

### Our Contribution (This Experiment)

**Novel Result**: The "correlation cliff" - a sharp phase transition where CC crosses 1.0 as dependence increases.

**Key Innovation**: We don't assume independence. Instead:
1. Fix per-guardrail marginals (TPR, FPR) based on isolated testing
2. Treat joint distribution as **unknown copula** constrained by Fr√©chet-Hoeffding (FH) bounds
3. Sweep dependence parameter Œª ‚àà [0,1] to scan the entire feasible space
4. Measure CC(Œª) empirically via two-world distinguishability tests
5. Identify Œª\* where CC(Œª\*) = 1 (the cliff)

**Why it's rigorous**:
- ‚úÖ Theory-backed: FH bounds are provably tight for marginal-constrained copulas
- ‚úÖ Statistically sound: BCa bootstrap + jackknife acceleration for proper inference
- ‚úÖ Falsifiable: If empirical J_C ever violates FH envelope, the theory is wrong (hasn't happened)
- ‚úÖ Reproducible: Deterministic RNG, config snapshots, cryptographic audit logs (in CC-Framework)

---

## Repository Structure

```
experiments/correlation_cliff/
‚îú‚îÄ‚îÄ config_s1.yaml              # Single source of truth for experiment config
‚îú‚îÄ‚îÄ theory.py                   # Mathematical core (FH bounds, metrics, closed forms)
‚îú‚îÄ‚îÄ simulate.py                 # Multinomial sampling, empirical estimates
‚îú‚îÄ‚îÄ analyze_bootstrap.py        # BCa confidence intervals (optional but recommended)
‚îú‚îÄ‚îÄ figures.py                  # Publication-grade plots
‚îú‚îÄ‚îÄ run_all.py                  # End-to-end orchestrator (THIS IS YOUR ENTRY POINT)
‚îú‚îÄ‚îÄ README.md                   # This file
‚îî‚îÄ‚îÄ artifacts/                  # Generated outputs (gitignored)
    ‚îî‚îÄ‚îÄ <timestamp>/
        ‚îú‚îÄ‚îÄ population_curve.csv
        ‚îú‚îÄ‚îÄ sim_long.csv
        ‚îú‚îÄ‚îÄ sim_summary.csv
        ‚îú‚îÄ‚îÄ thresholds.json
        ‚îú‚îÄ‚îÄ figures/
        ‚îÇ   ‚îú‚îÄ‚îÄ cc_vs_dependence.pdf
        ‚îÇ   ‚îú‚îÄ‚îÄ jc_fh_envelope.pdf
        ‚îÇ   ‚îú‚îÄ‚îÄ theory_vs_empirical.pdf
        ‚îÇ   ‚îî‚îÄ‚îÄ dependence_mapping.pdf
        ‚îî‚îÄ‚îÄ manifest.json
```

### What Each Module Does

#### `theory.py` (1200 lines)
**Purpose**: Pure mathematics - no randomness, no I/O. This is the "physics engine."

**Key functions**:
- `fh_bounds(pA, pB)`: Compute feasible interval for p11 given marginals
- `p11_path(pA, pB, Œª, path='fh_linear')`: Map Œª to p11 via chosen dependence path
- `compute_metrics_for_lambda(...)`: Population CC, J_C, dependence summaries at a given Œª
- `lambda_star_closed_form_fh_linear(...)`: Analytical Œª\* for FH-linear (when solvable)
- `compute_fh_jc_envelope(...)`: Min/max J_C feasible under FH constraints (sanity bound)

**Dependence paths**:
- `fh_linear`: p11(Œª) = L + Œª(U - L) (default, canonical)
- `fh_power`: p11(Œª) = L + Œª^k (U - L) (curved sweep)
- `fh_scurve`: Logistic-transformed sweep (sharp transition at Œª=0.5)
- `gaussian_copula`: Bernoulli margins via latent Gaussian (requires SciPy)

**Mathematical novelty**: 
- Proves J_C(Œª) is piecewise affine under FH-linear + no-sign-flip condition
- Implements Kendall œÑ_a for 2√ó2 tables (concordance functional)
- Provides delta-method variance formulas for fast analytic CIs

#### `simulate.py` (800 lines)
**Purpose**: Finite-sample validation via multinomial sampling.

**Key functions**:
- `simulate_grid(cfg)`: Run full Œª-grid with n_reps replicates each
- `simulate_replicate_at_lambda(...)`: Draw (N00, N01, N10, N11) ~ Multinomial(n; p) per world, compute hats
- `summarize_simulation(df_long)`: Aggregate replicates ‚Üí means, stds, quantiles

**What it outputs** (`sim_long.csv`):
- One row per (Œª, replicate)
- Columns: `n00_0, n01_0, n10_0, n11_0` (world 0 counts), same for world 1
- Empirical estimates: `CC_hat, JC_hat, phi_hat_avg, tau_hat_avg`
- Theory overlay: `CC_theory, JC_theory` (for comparison)
- Sanity flags: `JC_env_violation` (if empirical J_C outside FH envelope)

**Why multinomial?**: The observable data at each Œª is a 2√ó2 contingency table per world. Multinomial is the correct sampling distribution for fixed n.

#### `analyze_bootstrap.py` (700 lines)
**Purpose**: Rigorous uncertainty quantification via BCa bootstrap.

**Why BCa over percentile?**:
- CC is a ratio metric (J_C / J_best) ‚Üí often skewed
- BCa corrects for bias (z0) and skewness (acceleration a)
- Jackknife-based acceleration handles kinks from |ŒîC| structure

**Key functions**:
- `bca_for_one_lambda(...)`: Per-Œª BCa CI for CC_hat and JC_hat
- `threshold_bootstrap_percentile(...)`: Curve-level Œª\* bootstrap (resample entire curve)
- `_jackknife_acceleration_multinomial(...)`: Leave-one-out acceleration for contingency tables

**Outputs** (`bca_by_lambda.csv`):
- Per-Œª: `CC_hat_bca_lo, CC_hat_bca_hi` (and same for JC)
- Diagnostics: `z0` (bias), `a` (acceleration), `alpha1/alpha2` (adjusted quantiles)

**Usage** (optional, after `run_all.py`):
```bash
python analyze_bootstrap.py \
  --sim_long artifacts/<run>/sim_long.csv \
  --out artifacts/<run>/bca \
  --rep 0 --B 2000 --do_threshold_boot

# Then merge BCa columns into sim_summary.csv
# figures.py auto-prefers BCa columns if present
```

#### `figures.py` (400 lines)
**Purpose**: Turn tables into publication-grade PDFs.

**Generated figures**:
1. **`cc_vs_dependence.pdf`**: The money plot
   - CC(Œª) with empirical mean + CI band
   - Population theory overlay (dashed)
   - Neutrality band (CC ‚àà [0.95, 1.05])
   - Vertical line at Œª\* (if found)

2. **`jc_fh_envelope.pdf`**: J_C with feasibility envelope
   - Empirical J_C_hat vs Œª
   - Gray shaded region: FH envelope [J_min, J_max]
   - Validates: all points should lie within envelope

3. **`theory_vs_empirical.pdf`**: Error plot
   - (CC_hat_mean - CC_pop) vs Œª
   - Shows finite-sample bias (should center near 0)

4. **`dependence_mapping.pdf`**: Interpretability aid
   - phi_avg(Œª) and tau_avg(Œª) overlaid
   - Maps Œª\* to correlation/concordance summaries

**Style**: Clean, publication-ready, no chartjunk. Designed for copy-paste into LaTeX.

#### `run_all.py` (300 lines)
**Purpose**: One-command end-to-end orchestration.

**What it does** (in order):
1. Load YAML config ‚Üí resolve marginals, Œª-grid, sampling params
2. Compute **population curve** via theory.py (deterministic, fast)
3. Run **simulation** via simulate.py (Monte Carlo, slow)
4. Estimate **thresholds** (Œª\*, phi\*, tau\*) via interpolation
5. Generate **figures** via figures.py
6. Write **manifest** (timestamp, config snapshot, thresholds, figure paths)

**Output directory structure**:
```
artifacts/<timestamp>/
‚îú‚îÄ‚îÄ config_resolved.json      # What was actually run
‚îú‚îÄ‚îÄ population_curve.csv      # Theory curve (Œª, CC_pop, JC_pop, ...)
‚îú‚îÄ‚îÄ sim_long.csv              # Replicate-level (~4M rows for 201 Œª √ó 1 rep)
‚îú‚îÄ‚îÄ sim_summary.csv           # Per-Œª summary (~201 rows)
‚îú‚îÄ‚îÄ thresholds.json           # {lambda_star_pop: 0.47, lambda_star_emp: 0.48, ...}
‚îú‚îÄ‚îÄ figures/
‚îÇ   ‚îî‚îÄ‚îÄ (4 PDFs + optional PNGs)
‚îî‚îÄ‚îÄ manifest.json             # Reproducibility metadata
```

---

## Configuration (`config_s1.yaml`)

### Critical parameters you MUST set:

```yaml
marginals:
  w0:
    pA: 0.20    # YOUR S1 VALUE: P(A=1 | world 0)
    pB: 0.18    # YOUR S1 VALUE: P(B=1 | world 0)
  w1:
    pA: 0.62    # YOUR S1 VALUE: P(A=1 | world 1)
    pB: 0.55    # YOUR S1 VALUE: P(B=1 | world 1)
```

**How to set these**:
- **From ROC evaluation**: If you have TPR_A, FPR_A at a chosen threshold:
  - pA^0 = FPR_A (trigger rate on safe/world-0 samples)
  - pA^1 = TPR_A (trigger rate on unsafe/world-1 samples)
- **From two-world test**: Run your guardrail on two datasets (protected vs unprotected), measure trigger rates directly.

**Example (toxic content detection)**:
- World 0: Benign Reddit comments (baseline trigger rate)
- World 1: Known toxic samples from adversarial dataset
- Guardrail A (keyword filter): pA^0=0.15, pA^1=0.72
- Guardrail B (ML classifier): pB^0=0.20, pB^1=0.65

### Other key settings:

```yaml
composition:
  rule: "OR"    # or "AND"

dependence_path:
  type: "fh_linear"   # canonical baseline
  lambda_grid:
    num: 201          # resolution (higher = finer cliff measurement)

sampling:
  n_per_world: 20000  # sample size (higher = tighter CIs)
  seed: 20251220      # RNG seed for reproducibility

bootstrap:
  enabled: true
  B: 2000             # bootstrap reps (2000 is BCa standard)
  method: "bca"       # bias-corrected accelerated (vs "percentile")
```

**Computational cost**:
- Runtime scales as: O(n_lambda √ó n_reps √ó n_per_world √ó B_bootstrap)
- Default (201 Œª, 1 rep, 20k samples, 2k bootstrap): ~8 min CPU
- To speed up: reduce `lambda_grid.num` to 51 (~2 min) or disable bootstrap

---

## Understanding the Outputs

### Key Result: The Cliff

**Population curve** (`population_curve.csv`):
- `lambda`: Dependence parameter [0, 1]
- `CC_pop`: Exact CC at this Œª (deterministic, from theory)
- `JC_pop`: Exact J_C
- `phi_pop_avg`: Average phi coefficient (correlation proxy)
- `tau_pop_avg`: Average Kendall œÑ (concordance proxy)

**Simulation summary** (`sim_summary.csv`):
- `CC_hat_mean`: Empirical CC estimate (averaged over replicates)
- `CC_hat_q0025, CC_hat_q0975`: 95% quantile-based CI
- `CC_bca_lo, CC_bca_hi`: BCa CI (if bootstrap ran)
- Same for `JC_hat_*`

**Threshold** (`thresholds.json`):
```json
{
  "lambda_star_pop": 0.4723,     // Theory crossing
  "lambda_star_emp": 0.4801,     // Empirical crossing
  "phi_star_pop": 0.3156,        // Correlation at Œª*
  "tau_star_pop": 0.2089         // Kendall œÑ at Œª*
}
```

**Interpretation**:
- Œª\* ‚âà 0.47 means: "If dependence exceeds 47% of the FH range, composition becomes destructive"
- phi\* ‚âà 0.32 means: "Don't compose if correlation > 0.32"
- This is **actionable**: Measure correlation in production data ‚Üí decide whether to compose

### Sanity Checks (Validation)

**1. FH Envelope Check** (`jc_fh_envelope.pdf`):
- All empirical J_C points should lie within gray shaded region
- If violations occur: either (a) bug in code, or (b) FH theory is wrong (hasn't happened)
- Current status: 0 violations across all experiments

**2. Theory-Empirical Agreement** (`theory_vs_empirical.pdf`):
- Error should be small (<0.05) and centered near 0
- Large systematic bias indicates: (a) insufficient n, or (b) path mismatch

**3. Marginal Stability**:
- `sim_long.csv` includes `pA_hat_0, pA_hat_1` per replicate
- Should match config marginals within ~3œÉ (z-score check in theory.py)

---

## Extending This Experiment

### 1. Try Different Dependence Paths

**Current** (FH-linear): Uniform sweep through FH box - simple, interpretable, but "singular"

**Alternatives**:
```yaml
dependence_path:
  type: "gaussian_copula"
  # Maps Œª ‚àà [0,1] to œÑ ‚àà [-1,1] via Kendall's tau
  # More realistic for real guardrails (smooth dependence)
```

**Why it matters**: FH-linear is a "worst-case scanner" - it explores extreme anti-/co-monotonic dependence. Gaussian copula is smoother and may better match real correlation structures.

**Trade-off**: Gaussian path requires SciPy and is slower (~3x) due to bivariate normal CDF calls.

### 2. Multi-World Extensions

**Current**: Two worlds (0 vs 1) for simple leakage gap

**Extension**: k>2 worlds for richer distinguishability
- Example: World 0=benign, World 1=mild toxicity, World 2=severe toxicity
- Metric: Multi-class ROC AUC or polytope distinguishability

**Why**: Real systems face multi-class threats (PII, hate speech, violence, etc.)

### 3. Real-World Validation

**Synthetic marginals** (current): You set pA, pB by hand

**Real marginals**: Run CC-Framework on actual guardrail implementations:
```python
from cc_framework.adapters import NeMoGuardrailAdapter, OpenAIPolicyAdapter

rail_a = NeMoGuardrailAdapter("nemo://toxicity-v2")
rail_b = OpenAIPolicyAdapter("gpt-4-moderation")

# Evaluate on two datasets ‚Üí get empirical (pA^0, pA^1, pB^0, pB^1)
# Feed into correlation_cliff with those marginals
```

**Output**: Œª\* for REAL deployed system ‚Üí actionable audit report

### 4. Adversarial Correlation Attack (Option 3)

**Threat model**: Attacker crafts inputs that induce correlation between independent guardrails

**Experiment**:
1. Start with œÅ=0 (independent failures)
2. Use gradient descent to find inputs where both rails fail together
3. Measure CC before/after attack
4. Report: "Adversarial attack increased œÅ from 0.05 to 0.65 ‚Üí CC jumped from 0.82 to 1.4"

**Why brutal**: This is a CVE-worthy finding - shows composition is *attackable*

---

## Troubleshooting

### "RuntimeError: No crossing found"
**Cause**: CC never crosses 1.0 on your Œª-grid

**Fixes**:
1. Check marginals: If worlds are too similar (small J_A, J_B), CC stays <1 everywhere
2. Try opposite rule: If OR is always constructive, try AND
3. Increase Œª-grid resolution: `num: 401` instead of 201

### "ValueError: Counts do not sum to n"
**Cause**: Bug in multinomial sampling (shouldn't happen with current code)

**Fix**: Verify `n_per_world > 0` in config and no NaN marginals

### "Figures look weird (jagged, discontinuous)"
**Cause**: Insufficient Œª-grid resolution near the cliff

**Fix**: Enable adaptive refinement:
```yaml
dependence_path:
  refine:
    enabled: true
    half_width: 0.08   # Refine ¬±0.08 around crossing
    num: 401           # Dense grid in refined region
```

### "Bootstrap taking forever (>30 min)"
**Cause**: B=2000 √ó 201 Œª-points √ó expensive metric

**Fixes**:
1. Reduce B to 1000 (still valid, just wider CIs)
2. Disable curve-level threshold bootstrap: `threshold.enabled: false`
3. Parallelize (not implemented yet - single-threaded by design for reproducibility)

---

## Citation & Attribution

If you use this experiment in a paper, cite:

```bibtex
@software{correlation_cliff_2025,
  author = {YOUR NAME},
  title = {Correlation Cliff: Empirical Phase Transitions in Guardrail Composition},
  year = {2025},
  url = {https://github.com/YOUR_REPO/CC-Framework},
  note = {Part of the Composability Coefficient (CC) framework for AI safety}
}
```

**Related work to cite**:
- Fr√©chet-Hoeffding bounds: Fr√©chet (1951), Hoeffding (1940)
- BCa bootstrap: Efron & Tibshirani (1993), *An Introduction to the Bootstrap*
- Kendall's œÑ: Kendall (1938), "A New Measure of Rank Correlation"

---

## FAQ

**Q: Why FH-linear as default path?**
A: It's the simplest dependence scan that respects marginal constraints. Covers the entire feasible range [L, U] uniformly. Other paths (power, s-curve, Gaussian) are refinements but FH-linear is the canonical baseline.

**Q: What if I only care about independence (œÅ=0)?**
A: Set `lambda_grid: {num: 3}` with Œª ‚àà {0.0, 0.5, 1.0}. Compare CC at Œª=0.5 (independence under FH-linear) vs Œª=0.0/1.0 (extremes). But you're missing the cliff discovery.

**Q: Can I use this for non-binary guardrails (e.g., confidence scores)?**
A: Not directly. Current theory assumes binary triggers (flag vs no-flag). Extension to soft scores requires copula-based composition theory (planned for CC-Framework v2).

**Q: What's the difference between this and just testing composed guardrails?**
A: Standard testing gives you ONE point (whatever the true œÅ is). This experiment sweeps the entire dependence space to find WHERE composition breaks. You're mapping risk, not just measuring it.

**Q: How do I know my Œª\* estimate is trustworthy?**
A: Check:
1. BCa CI width: Œª\* ¬± 0.03 is tight, ¬± 0.15 is loose
2. Theory-empirical gap: Should be <0.02 near Œª\*
3. Bootstrap distribution: Should be unimodal (check `threshold_bootstrap.json`)

**Q: Can I run this on a cluster?**
A: Yes, but current code is single-threaded by design (reproducibility via fixed seed). To parallelize:
1. Split Œª-grid across workers (each gets its own seed offset)
2. Aggregate results post-hoc
3. Careful: bootstrap RNG must remain deterministic per Œª

---

## Next Steps (Your Roadmap)

**Immediate (Days 1-3)**:
1. ‚úÖ Run `python run_all.py --config config_s1.yaml` with YOUR S1 marginals
2. ‚úÖ Generate figures ‚Üí review cc_vs_dependence.pdf
3. ‚úÖ Check Œª\* and phi\* ‚Üí interpret for your use case
4. ‚úÖ Run BCa bootstrap for publication-grade CIs

**Short-term (Week 1)**:
1. Test sensitivity: Re-run with n=5000, 10000, 40000 ‚Üí verify Œª\* convergence
2. Try gaussian_copula path ‚Üí compare cliff location vs FH-linear
3. Vary composition rule: Run with AND ‚Üí compare to OR results
4. Write up Methods section for paper using manifest.json

**Medium-term (Weeks 2-4)**:
1. Real-world validation: Pick 2 deployed guardrails, measure their marginals, run experiment
2. Multi-rail extension: Implement n=3 composition (A AND B AND C)
3. Adversarial attack: Implement gradient-based correlation induction
4. Comparison study: Compare Œª\* across different guardrail pairs

**Long-term (Months 1-3)**:
1. Full paper draft: Introduction, Methods (this experiment), Results, Discussion
2. Case studies: 3-5 real systems audited with CC-Framework
3. Tool release: Package as `cc-audit` CLI tool for practitioners
4. Conference submission: ICLR/NeurIPS (ML) or IEEE S&P (security)

---

## Support & Contact

**Issues**: Open a GitHub issue with:
- `config_s1.yaml` (sanitized if needed)
- Error message + full traceback
- `manifest.json` from failed run

**Questions**: Tag @YOUR_HANDLE or email YOUR_EMAIL

**Contributing**: PRs welcome for:
- New dependence paths (e.g., Frank copula, Clayton copula)
- Parallelization (multi-core bootstrap)
- GPU acceleration for large-n sampling
- Additional sanity checks / diagnostics

---

## Appendix: Mathematical Definitions

### Composability Coefficient (CC)

Given two binary guardrails A and B operating in two worlds (w=0, w=1):

**Leakage** (two-world distinguishability gap):
```
J_X = |P(X=1|w=1) - P(X=1|w=0)|  for X ‚àà {A, B, C}
```

**Composition**: C = f(A,B) where f ‚àà {OR, AND}

**Baseline**: J_best = max(J_A, J_B)

**CC**:
```
CC = J_C / J_best
```

**Interpretation**:
- CC < 1: Composition reduces leakage vs best singleton (constructive)
- CC = 1: Composition matches best singleton (neutral)
- CC > 1: Composition increases leakage vs best singleton (destructive)

### Fr√©chet-Hoeffding Bounds

Given marginals P(A=1)=p_A, P(B=1)=p_B, the overlap p_11 = P(A=1,B=1) satisfies:

```
L ‚â§ p_11 ‚â§ U

L = max(0, p_A + p_B - 1)   (comonotonic lower bound)
U = min(p_A, p_B)           (countermonotonic upper bound)
```

**Why this matters**: Without knowing the joint distribution, p_11 can be ANYWHERE in [L,U]. This is the "unknown copula problem."

### FH-Linear Path

A minimal parameterization for scanning [L,U]:

```
p_11(Œª) = L + Œª(U - L),   Œª ‚àà [0,1]
```

**Geometric interpretation**: Straight line from L to U in the feasible p_11 interval.

**Copula interpretation**: This is a "singular" path (not a smooth copula family), but it's the simplest way to explore the entire feasible space uniformly.

### Phi Coefficient

Binary Pearson correlation for 2√ó2 tables:

```
œÜ = (p_11 - p_A p_B) / sqrt(p_A(1-p_A) p_B(1-p_B))
```

**Range**: œÜ ‚àà [-1, +1]

**Interpretation**:
- œÜ > 0: Failures co-occur more than independence predicts
- œÜ = 0: Independent failures
- œÜ < 0: Failures anti-correlated (one fails ‚Üí other unlikely to fail)

### Kendall's œÑ_a (Binary Case)

Population concordance measure:

```
œÑ_a = 2(p_00 p_11 - p_01 p_10)
```

**Derivation**: For i.i.d. pairs (A_i, B_i), (A_j, B_j), œÑ_a = P(concordant) - P(discordant) where concordant means (A_i < A_j and B_i < B_j) or (A_i > A_j and B_i > B_j).

**Why use œÑ instead of œÜ?**: œÑ is invariant to monotone transformations (copula-theoretic). œÜ is specific to linear correlation. For dependence quantification, œÑ is often preferred.

---

**Document version**: 2025-12-20  
**Experiment status**: Production-ready, awaiting YOUR S1 marginals  
**Next artifact**: Multi-rail scaling law (Option 2)
